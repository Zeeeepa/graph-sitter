# Continuous Learning Integration Test Configuration
# This configuration file defines the testing infrastructure and parameters
# for comprehensive integration testing as specified in ZAM-1053

testing:
  environment:
    database_size: "production_scale"
    concurrent_users: 1000
    test_duration: "72_hours"  # For long-running tests
    data_retention: "1_year"
    
  performance_targets:
    response_time_p95: 2000  # milliseconds
    error_rate: 0.1          # percentage
    availability: 99.9       # percentage
    mttr: 300               # seconds (5 minutes)
    
  load_testing:
    ramp_up_duration: 300    # seconds (5 minutes)
    steady_state_duration: 3600  # seconds (1 hour)
    scenarios:
      - "normal_operations"
      - "high_load"
      - "error_injection"
      - "component_failure"
    
    user_profiles:
      - name: "light_user"
        percentage: 60
        requests_per_minute: 10
      - name: "moderate_user"
        percentage: 30
        requests_per_minute: 30
      - name: "heavy_user"
        percentage: 10
        requests_per_minute: 100

# Component-specific test configurations
components:
  openevolve:
    api_url: "http://localhost:8080/api/v1"
    timeout: 30
    max_retries: 3
    evaluation_types:
      - "code_quality"
      - "performance_optimization"
      - "error_analysis"
      - "pattern_recognition"
    
  self_healing:
    detection_threshold: 0.8
    recovery_timeout: 300  # seconds
    escalation_timeout: 900  # seconds
    error_types:
      - "database_connection_timeout"
      - "memory_leak"
      - "api_rate_limit_exceeded"
      - "service_unavailable"
      - "performance_degradation"
    
  pattern_analysis:
    model_types:
      - "time_series_analysis"
      - "anomaly_detection"
      - "predictive_modeling"
      - "clustering"
    accuracy_threshold: 0.80
    processing_timeout: 30  # seconds
    
  database:
    connection_pool_size: 20
    query_timeout: 10  # seconds
    backup_retention: "30_days"
    modules:
      - "user_management"
      - "system_metrics"
      - "error_logs"
      - "learning_data"
      - "pattern_cache"
      - "configuration"
      - "audit_trail"

# Test scenarios and data
test_scenarios:
  normal_operations:
    description: "Standard system operation under normal load"
    duration: 1800  # 30 minutes
    user_load: 100
    error_injection: false
    
  high_load:
    description: "System operation under high load conditions"
    duration: 3600  # 1 hour
    user_load: 1000
    error_injection: false
    
  error_injection:
    description: "System behavior with injected errors"
    duration: 1800  # 30 minutes
    user_load: 200
    error_injection: true
    error_types:
      - "network_latency"
      - "database_timeout"
      - "service_failure"
      - "memory_pressure"
    
  component_failure:
    description: "System resilience with component failures"
    duration: 2400  # 40 minutes
    user_load: 300
    error_injection: true
    failure_types:
      - "database_unavailable"
      - "openevolve_api_down"
      - "pattern_analysis_crash"
      - "self_healing_disabled"

# Success criteria (from ZAM-1053 requirements)
success_criteria:
  integration_tests:
    success_rate: 0.95  # >95% success rate
    
  performance:
    response_time_p95: 2000  # <2 second response times
    concurrent_users: 1000   # Handle 1000+ concurrent users
    error_rate: 0.1         # <0.1% error rate
    availability: 99.9      # 99.9% availability
    
  continuous_learning:
    improvement_period: "30_days"  # Measurable improvements over 30-day period
    learning_effectiveness: 0.80   # 80% learning effectiveness
    
  error_recovery:
    mttr: 300              # <5 minute MTTR for common issues
    detection_rate: 0.95   # 95% error detection rate
    recovery_rate: 0.70    # 70% automated recovery rate
    
  pattern_analysis:
    accuracy: 0.80         # >80% accuracy in predictions and recommendations
    recommendation_relevance: 0.85  # 85% recommendation relevance
    
  production_readiness:
    deployment_success_rate: 0.95
    monitoring_coverage: 0.90
    backup_recovery_success: 0.99
    security_compliance: 0.95

# Monitoring and alerting configuration
monitoring:
  health_checks:
    interval: 30  # seconds
    timeout: 10   # seconds
    endpoints:
      - "/health"
      - "/api/v1/status"
      - "/metrics"
      - "/ready"
      
  metrics:
    collection_interval: 15  # seconds
    retention_period: "7_days"
    types:
      - "response_time"
      - "error_rate"
      - "throughput"
      - "cpu_usage"
      - "memory_usage"
      - "disk_usage"
      - "network_io"
      
  alerts:
    response_time_threshold: 2000  # milliseconds
    error_rate_threshold: 0.05     # 5%
    cpu_usage_threshold: 80        # percentage
    memory_usage_threshold: 85     # percentage
    disk_usage_threshold: 90       # percentage

# Data generation for testing
test_data:
  historical_data:
    time_range: "90_days"
    data_points_per_day: 1440  # Every minute
    metrics:
      - "response_time"
      - "error_rate"
      - "user_count"
      - "cpu_usage"
      - "memory_usage"
      
  synthetic_data:
    patterns:
      - type: "daily_cycle"
        amplitude: 0.3
        period: 86400  # 24 hours
      - type: "weekly_cycle"
        amplitude: 0.2
        period: 604800  # 7 days
      - type: "random_noise"
        amplitude: 0.1
        
  error_scenarios:
    - type: "gradual_degradation"
      duration: 3600  # 1 hour
      severity: "medium"
    - type: "sudden_spike"
      duration: 300   # 5 minutes
      severity: "high"
    - type: "intermittent_failures"
      duration: 1800  # 30 minutes
      severity: "low"

# Reporting configuration
reporting:
  output_formats:
    - "json"
    - "html"
    - "csv"
    
  metrics_to_report:
    - "test_execution_summary"
    - "performance_benchmarks"
    - "success_criteria_validation"
    - "component_health_status"
    - "error_analysis"
    - "recommendations"
    
  dashboard:
    enabled: true
    refresh_interval: 30  # seconds
    charts:
      - "response_time_trends"
      - "error_rate_over_time"
      - "system_resource_usage"
      - "test_success_rates"
      - "learning_effectiveness"

# Environment-specific overrides
environments:
  development:
    testing:
      environment:
        concurrent_users: 10
        test_duration: "30_minutes"
      performance_targets:
        response_time_p95: 5000  # More lenient for dev
        
  staging:
    testing:
      environment:
        concurrent_users: 100
        test_duration: "2_hours"
      performance_targets:
        response_time_p95: 3000  # Slightly more lenient for staging
        
  production:
    testing:
      environment:
        concurrent_users: 1000
        test_duration: "72_hours"
      performance_targets:
        response_time_p95: 2000  # Full production requirements

