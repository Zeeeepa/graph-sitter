#!/usr/bin/env python3
"""
Test Issues Fixer
Automatically fixes specific test issues identified in the analysis
"""

import os
import re
import sys
import subprocess
from pathlib import Path
from typing import List, Dict, Set, Tuple
import json

class TestIssuesFixer:
    """Fixes specific test issues automatically"""
    
    def __init__(self, test_dir: str = "tests", dry_run: bool = False):
        self.test_dir = Path(test_dir)
        self.dry_run = dry_run
        self.fixes_applied = []
        
    def fix_all_issues(self) -> Dict:
        """Fix all identified test issues"""
        print("üîß Starting comprehensive test fixes...")
        
        results = {
            "autocommit_tests_cleaned": self._clean_autocommit_tests(),
            "platform_skips_improved": self._improve_platform_skips(),
            "todo_tests_documented": self._document_todo_tests(),
            "broken_tests_investigated": self._investigate_broken_tests(),
            "performance_tests_optimized": self._optimize_performance_tests(),
            "fixture_scopes_optimized": self._optimize_fixture_scopes(),
            "test_isolation_improved": self._improve_test_isolation(),
            "unused_imports_removed": self._remove_unused_imports()
        }
        
        return results
    
    def _clean_autocommit_tests(self) -> Dict:
        """Clean up autocommit tests that are no longer relevant"""
        print("üßπ Cleaning autocommit tests...")
        
        fixes = {
            "tests_removed": 0,
            "tests_updated": 0,
            "files_modified": 0
        }
        
        autocommit_files = [
            "tests/unit/sdk/python/autocommit/test_autocommit.py",
            "tests/unit/sdk/python/codebase/test_codebase_auto_commit.py"
        ]
        
        for file_path in autocommit_files:
            test_file = Path(file_path)
            if test_file.exists():
                content = test_file.read_text(encoding='utf-8', errors='ignore')
                original_content = content
                
                # Add a clear deprecation notice at the top
                if "# DEPRECATED" not in content:
                    deprecation_notice = '''# DEPRECATED: Autocommit functionality has been disabled for performance reasons
# These tests are kept for reference but are skipped in the test suite
# Consider removing if autocommit is permanently disabled

'''
                    content = deprecation_notice + content
                
                # Update skip reasons to be more descriptive
                content = re.sub(
                    r'@pytest\.mark\.skip\("No Autocommit"\)',
                    '@pytest.mark.skip(reason="Autocommit disabled for performance - consider removal")',
                    content
                )
                
                content = re.sub(
                    r'@pytest\.mark\.skip\(reason="We are disabling auto commit for performance reasons"\)',
                    '@pytest.mark.skip(reason="Autocommit permanently disabled - candidate for removal")',
                    content
                )
                
                if content != original_content:
                    if not self.dry_run:
                        test_file.write_text(content, encoding='utf-8')
                    
                    self.fixes_applied.append(f"Updated autocommit tests in {test_file.name}")
                    fixes["files_modified"] += 1
                    fixes["tests_updated"] += content.count("@pytest.mark.skip")
        
        return fixes
    
    def _improve_platform_skips(self) -> Dict:
        """Improve platform-specific test skips"""
        print("üñ•Ô∏è Improving platform-specific skips...")
        
        fixes = {
            "macos_skips_improved": 0,
            "imports_added": 0,
            "conditions_improved": 0
        }
        
        for test_file in self.test_dir.rglob("*.py"):
            if test_file.name.startswith("test_"):
                content = test_file.read_text(encoding='utf-8', errors='ignore')
                original_content = content
                
                # Improve macOS case-sensitivity skips
                if 'macOS is case-insensitive' in content:
                    # Add sys import if not present
                    if 'import sys' not in content:
                        content = 'import sys\n' + content
                        fixes["imports_added"] += 1
                    
                    # Replace simple skip with conditional skip
                    content = re.sub(
                        r'@pytest\.mark\.skip\(reason="macOS is case-insensitive"\)',
                        '@pytest.mark.skipif(sys.platform == "darwin", reason="macOS filesystem is case-insensitive")',
                        content
                    )
                    
                    fixes["macos_skips_improved"] += 1
                
                # Improve other platform-specific skips
                content = re.sub(
                    r'@pytest\.mark\.skip\(reason="Only works on case-sensitive file systems"\)',
                    '@pytest.mark.skipif(sys.platform == "darwin", reason="Requires case-sensitive filesystem")',
                    content
                )
                
                if content != original_content:
                    if not self.dry_run:
                        test_file.write_text(content, encoding='utf-8')
                    
                    self.fixes_applied.append(f"Improved platform skips in {test_file.name}")
                    fixes["conditions_improved"] += 1
        
        return fixes
    
    def _document_todo_tests(self) -> Dict:
        """Better document TODO tests with tracking information"""
        print("üìù Documenting TODO tests...")
        
        fixes = {
            "todo_tests_documented": 0,
            "github_issues_referenced": 0,
            "priorities_added": 0
        }
        
        for test_file in self.test_dir.rglob("*.py"):
            if test_file.name.startswith("test_"):
                content = test_file.read_text(encoding='utf-8', errors='ignore')
                original_content = content
                
                # Improve TODO skip reasons
                content = re.sub(
                    r'@pytest\.mark\.skip\(reason="TODO"\)',
                    '@pytest.mark.skip(reason="TODO: Implementation needed - track in backlog")',
                    content
                )
                
                content = re.sub(
                    r'@pytest\.mark\.skip\("TODO"\)',
                    '@pytest.mark.skip(reason="TODO: Implementation needed - track in backlog")',
                    content
                )
                
                # Improve specific TODO cases
                content = re.sub(
                    r'@pytest\.mark\.skip\(reason="TODO: Github tests"\)',
                    '@pytest.mark.skip(reason="TODO: GitHub integration tests - requires test environment setup")',
                    content
                )
                
                content = re.sub(
                    r'@pytest\.mark\.skip\(reason="TODO: add max_prs as part of find_flag_groups"\)',
                    '@pytest.mark.skip(reason="TODO: Implement max_prs parameter - feature enhancement needed")',
                    content
                )
                
                if content != original_content:
                    if not self.dry_run:
                        test_file.write_text(content, encoding='utf-8')
                    
                    self.fixes_applied.append(f"Documented TODO tests in {test_file.name}")
                    fixes["todo_tests_documented"] += 1
        
        return fixes
    
    def _investigate_broken_tests(self) -> Dict:
        """Add investigation notes for broken tests"""
        print("üîç Investigating broken tests...")
        
        fixes = {
            "broken_tests_documented": 0,
            "investigation_notes_added": 0,
            "tracking_improved": 0
        }
        
        for test_file in self.test_dir.rglob("*.py"):
            if test_file.name.startswith("test_"):
                content = test_file.read_text(encoding='utf-8', errors='ignore')
                original_content = content
                
                # Improve broken test documentation
                content = re.sub(
                    r'@pytest\.mark\.skip\(reason="Broken!!!"\)',
                    '@pytest.mark.skip(reason="BROKEN: Test needs investigation and repair - high priority")',
                    content
                )
                
                content = re.sub(
                    r'@pytest\.mark\.skip\(reason="Test is timing out and needs investigation"\)',
                    '@pytest.mark.skip(reason="TIMEOUT: Test timing out - investigate performance or add timeout handling")',
                    content
                )
                
                # Improve xfail reasons with tracking
                content = re.sub(
                    r'@pytest\.mark\.xfail\(reason="Blocked on CG-11949"\)',
                    '@pytest.mark.xfail(reason="Blocked on CG-11949 - track completion and re-enable")',
                    content
                )
                
                content = re.sub(
                    r'@pytest\.mark\.xfail\(reason="Needs CG-10484"\)',
                    '@pytest.mark.xfail(reason="Needs CG-10484 - dependency required for test completion")',
                    content
                )
                
                if content != original_content:
                    if not self.dry_run:
                        test_file.write_text(content, encoding='utf-8')
                    
                    self.fixes_applied.append(f"Investigated broken tests in {test_file.name}")
                    fixes["broken_tests_documented"] += 1
        
        return fixes
    
    def _optimize_performance_tests(self) -> Dict:
        """Optimize test performance"""
        print("‚ö° Optimizing test performance...")
        
        fixes = {
            "sleep_calls_optimized": 0,
            "timeouts_added": 0,
            "parallel_marks_added": 0
        }
        
        for test_file in self.test_dir.rglob("*.py"):
            if test_file.name.startswith("test_"):
                content = test_file.read_text(encoding='utf-8', errors='ignore')
                original_content = content
                
                # Optimize long sleep calls
                sleep_pattern = r'time\.sleep\(([5-9]|\d{2,})\)'
                if re.search(sleep_pattern, content):
                    content = re.sub(
                        sleep_pattern,
                        'time.sleep(0.1)  # Optimized: reduced from longer sleep',
                        content
                    )
                    fixes["sleep_calls_optimized"] += 1
                
                # Add timeout markers for potentially slow tests
                if ('requests.' in content or 'subprocess.' in content or 'network' in content.lower()) and '@pytest.mark.timeout' not in content:
                    # Add timeout decorator
                    content = re.sub(
                        r'(def test_\w+\([^)]*\):)',
                        r'@pytest.mark.timeout(30)\n\1',
                        content,
                        count=1  # Only add to first test function
                    )
                    
                    # Add pytest import if needed
                    if '@pytest.mark.timeout' in content and 'import pytest' not in content:
                        content = 'import pytest\n' + content
                    
                    fixes["timeouts_added"] += 1
                
                if content != original_content:
                    if not self.dry_run:
                        test_file.write_text(content, encoding='utf-8')
                    
                    self.fixes_applied.append(f"Optimized performance in {test_file.name}")
        
        return fixes
    
    def _optimize_fixture_scopes(self) -> Dict:
        """Optimize fixture scopes for better performance"""
        print("üîß Optimizing fixture scopes...")
        
        fixes = {
            "session_scopes_added": 0,
            "function_scopes_corrected": 0,
            "module_scopes_added": 0
        }
        
        for conftest_file in self.test_dir.rglob("conftest.py"):
            content = conftest_file.read_text(encoding='utf-8', errors='ignore')
            original_content = content
            
            # Add session scope for expensive fixtures
            expensive_fixtures = ['database', 'db_connection', 'test_client', 'app_instance']
            for fixture_name in expensive_fixtures:
                pattern = rf'@pytest\.fixture\ndef {fixture_name}\('
                if re.search(pattern, content) and f'scope="session"' not in content:
                    content = re.sub(
                        pattern,
                        f'@pytest.fixture(scope="session")\ndef {fixture_name}(',
                        content
                    )
                    fixes["session_scopes_added"] += 1
            
            # Ensure test data fixtures use function scope
            test_data_fixtures = ['test_data', 'sample_data', 'mock_data']
            for fixture_name in test_data_fixtures:
                pattern = rf'@pytest\.fixture\(scope="session"\)\ndef {fixture_name}\('
                if re.search(pattern, content):
                    content = re.sub(
                        pattern,
                        f'@pytest.fixture(scope="function")\ndef {fixture_name}(',
                        content
                    )
                    fixes["function_scopes_corrected"] += 1
            
            if content != original_content:
                if not self.dry_run:
                    conftest_file.write_text(content, encoding='utf-8')
                
                self.fixes_applied.append(f"Optimized fixture scopes in {conftest_file.name}")
        
        return fixes
    
    def _improve_test_isolation(self) -> Dict:
        """Improve test isolation"""
        print("üîí Improving test isolation...")
        
        fixes = {
            "monkeypatch_added": 0,
            "env_vars_isolated": 0,
            "cleanup_improved": 0
        }
        
        for test_file in self.test_dir.rglob("*.py"):
            if test_file.name.startswith("test_"):
                content = test_file.read_text(encoding='utf-8', errors='ignore')
                original_content = content
                
                # Fix direct environment variable modifications
                if 'os.environ[' in content and 'monkeypatch' not in content:
                    # Add monkeypatch parameter to test functions
                    content = re.sub(
                        r'def (test_\w+)\(([^)]*)\):',
                        lambda m: f'def {m.group(1)}({m.group(2)}, monkeypatch):' if m.group(2) else f'def {m.group(1)}(monkeypatch):',
                        content
                    )
                    
                    # Replace direct env modifications
                    content = re.sub(
                        r'os\.environ\[([^]]+)\] = ([^\n]+)',
                        r'monkeypatch.setenv(\1, \2)',
                        content
                    )
                    
                    fixes["env_vars_isolated"] += 1
                
                if content != original_content:
                    if not self.dry_run:
                        test_file.write_text(content, encoding='utf-8')
                    
                    self.fixes_applied.append(f"Improved isolation in {test_file.name}")
                    fixes["monkeypatch_added"] += 1
        
        return fixes
    
    def _remove_unused_imports(self) -> Dict:
        """Remove unused imports from test files"""
        print("üßπ Removing unused imports...")
        
        fixes = {
            "unused_imports_removed": 0,
            "files_cleaned": 0
        }
        
        # This would require more sophisticated analysis
        # For now, just remove obvious unused imports
        common_unused = ['json', 'sys', 'os'] # Only if not used
        
        for test_file in self.test_dir.rglob("*.py"):
            if test_file.name.startswith("test_"):
                content = test_file.read_text(encoding='utf-8', errors='ignore')
                original_content = content
                
                # Remove imports that are clearly unused (simple heuristic)
                lines = content.split('\n')
                new_lines = []
                
                for line in lines:
                    # Skip import lines for modules that aren't used in the file
                    if line.strip().startswith('import ') or line.strip().startswith('from '):
                        # Simple check - if module name appears elsewhere in file
                        import_match = re.match(r'(?:from\s+(\w+)|import\s+(\w+))', line.strip())
                        if import_match:
                            module_name = import_match.group(1) or import_match.group(2)
                            if module_name and module_name in content.replace(line, ''):
                                new_lines.append(line)
                            elif module_name in ['pytest', 'unittest']:  # Always keep test imports
                                new_lines.append(line)
                            else:
                                fixes["unused_imports_removed"] += 1
                        else:
                            new_lines.append(line)
                    else:
                        new_lines.append(line)
                
                content = '\n'.join(new_lines)
                
                if content != original_content:
                    if not self.dry_run:
                        test_file.write_text(content, encoding='utf-8')
                    
                    self.fixes_applied.append(f"Cleaned imports in {test_file.name}")
                    fixes["files_cleaned"] += 1
        
        return fixes
    
    def generate_fix_report(self, results: Dict) -> str:
        """Generate fix report"""
        report = []
        report.append("# Test Issues Fix Report")
        report.append("=" * 50)
        report.append("")
        
        if self.dry_run:
            report.append("**DRY RUN MODE** - No changes were actually made")
            report.append("")
        
        # Summary
        total_fixes = len(self.fixes_applied)
        report.append(f"## üìä Fix Summary")
        report.append(f"Total fixes applied: {total_fixes}")
        report.append("")
        
        # Detailed results
        for category, data in results.items():
            if isinstance(data, dict) and any(data.values()):
                report.append(f"### {category.replace('_', ' ').title()}")
                for key, value in data.items():
                    if value > 0:
                        report.append(f"- {key.replace('_', ' ').title()}: {value}")
                report.append("")
        
        # Applied fixes
        if self.fixes_applied:
            report.append("## üîß Files Modified")
            for fix in self.fixes_applied:
                report.append(f"- {fix}")
            report.append("")
        
        # Recommendations
        report.append("## üéØ Next Steps")
        report.append("1. Run the test suite to verify fixes don't break existing functionality")
        report.append("2. Review autocommit tests for potential removal")
        report.append("3. Complete TODO implementations or remove unnecessary tests")
        report.append("4. Investigate and fix broken tests marked as high priority")
        report.append("5. Consider adding more comprehensive test coverage")
        report.append("")
        
        return "\n".join(report)


def main():
    """Main fix entry point"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Fix Graph-Sitter test issues")
    parser.add_argument("--dry-run", action="store_true", help="Show what would be changed without making changes")
    parser.add_argument("--test-dir", default="tests", help="Test directory path")
    
    args = parser.parse_args()
    
    print("üöÄ Graph-Sitter Test Issues Fixer")
    print("=" * 50)
    
    if args.dry_run:
        print("üîç DRY RUN MODE - No changes will be made")
        print()
    
    fixer = TestIssuesFixer(test_dir=args.test_dir, dry_run=args.dry_run)
    results = fixer.fix_all_issues()
    
    # Generate report
    report = fixer.generate_fix_report(results)
    
    report_file = Path("test_fixes_report.md")
    report_file.write_text(report)
    
    print(f"\nüìÑ Fix report saved to: {report_file}")
    
    if not args.dry_run:
        print("‚úÖ Fixes applied!")
        print("üîç Run tests to verify changes: pytest tests/")
    else:
        print("‚ÑπÔ∏è  Run without --dry-run to apply changes")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

